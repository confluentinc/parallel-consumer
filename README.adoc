
//
// STOP!!! Make sure you're editing the TEMPLATE version of the README, in /src/docs/README.adoc
//
// Do NOT edit /README.adoc as your changes will be overwritten when the template is rendered again during
// process-sources.
//


= async-consumer (alpha)
:icons:
:toc: macro
:toclevels: 3
:numbered: 1

// dynamic include base for editing in IDEA
:project_root: ./

// uncomment the following if not using IDEA or having issues, for editing the template to see the includes
//:project_root: ../../


ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

image:https://travis-ci.com/astubbs/async-consumer.svg?branch=master["Build Status", link="https://travis-ci.com/astubbs/async-consumer"] image:https://codecov.io/gh/astubbs/async-consumer/branch/master/graph/badge.svg["Coverage",https://codecov.io/gh/astubbs/async-consumer]

Parallel Apache Kafka client wrapper with client side queueing, a simpler consumer/producer API with *key concurrency* and *extendable non-blocking IO* processing.

[[intro]]
Process your Kafka messages _orders of magnitudes *faster*_, with much lower latency, *without* adjusting your partitions or managing more client instances.

As processing speed and response time isn't bound to the number of partitions in a topic anymore, go faster in situations where partition counts are fixed such as legacy, Y, Z or where brokers are overloaded with too many partitions already or where the partition count is out of your control, or where running large consumer groups, or many consumer clients isn't desirable.

.Consume many messages _concurrently_ with a *single* consumer instance:
[source,java,indent=0]
----
        parallelConsumer.asyncPoll(record -> {
            log.info("Concurrently processing a record: {}", record);
        });
----


toc::[]

== Motivation

Process your Kafka messages _orders of magnitudes **faster**_, with much lower latency, *without* adjusting your partitions or managing more client instances.

more flexible use of resources

lower minimum and average message response times

=== Why would I need this?


Dramatically increased performance, lower latency, and save developers months of work.

Save money on shorter development time, save money by not needing to add resources or reduce resources deployed, achieve high-performance in under provisioned or miss-configured environments (partition counts), without unnecessarily burdening your Kafka Broker side with extra partition load or large consumer groups.

Save months of error prone development implementing this correctly yourself.

When reading the below, keep in mind that the unit of concurrency and thus performance, is restricted by the number of partitions (degree of sharding / concurrency).
Currently you can't adjust the number of partitions in your kafka topics without jumping through a lot of hoops, or breaking your key ordering.

[qanda]
Why not just run more consumers?::
The typical way to address performance issues in a Kafka system, is to increase the number of consumers reading from a topic. This is effective in many situations, but falls short in a lot too.
* Primarily: You cannot use more consumers than you have partitions available to read from.
+
For example, if you have a topic with five partitions, you cannot use more than five consumers to read from it.

** You can use this library to process messages concurrently up to the number of keys available, without worrying about increasing partition counts.
** You can increase partition counts by first copying all topic data to a new topic, but that doubles the data requirements for the topic, adds another processing step that has to be monitored, and increases minimum latency in end to end processing.
Then your level of concurrency (and thus latency and throughput) is then _still_ restricted to the number of partitions and consumer instances available.
** Large numbers of partitions in a Kafka broker system is one of the primary causes of instability and performance problems with over burdened cluster systems.
* Running more extra consumers has resource implications - each consumer takes up resources on both the client  and broker side. Each consumer adds a lot of overhead in terms of memory, CPU, and network bandwidth.
* Large consumer groups (especially many large groups) can cause a lot of strain on the consumer group coordination system, such as rebalance storms.
* Even with several partitions, you cannot achieve the performance levels obtainable by *per-key* ordered or unordered concurrent processing

Why not run more consumers __within__ your application instance?::
* This is in some respects a slightly easier way of running more consumer instances, and in others a more complicated way. However, you are still restricted by all the per consumer restrictions as described above.

Why not use the Vert.x library yourself in your processing loop?::
* Vert.x us used in this library to provide a non-blocking IO system in the message processing step.
Using Vert.x without using this library with *ordered* processing requires dealing with the quite complicated, and not straight forward, aspect of handling offset commits with Vert.x asynchronous processing system.
+
*Unordered* processing with Vert.x is somewhat easier, however offset management is still quite complicated, to  ensure that offsets are not committed, which would cause other not-yet-processed offsets to be incorrectly committed.
This library handles offset commits for both ordered and unordered processing cases.

=== Scenarios

Below are some real world use cases which illustrate concrete situations where the described advantages massively improve performance.

* Slow consumer systems in transactional systems (online vs offline or reporting systems)
** Notification system:
+
*** Notification processing system which sends push notifications to a user to acknowledge a two-factor authentication request on their mobile and authorizing a login to a website, requires optimal end-to-end latency for a good user experience.
*** A specific message in this queue uncharacteristically takes a long time to process because the third party system is sometimes unpredictably slow to respond and so holds up the processing for *ALL* other notifications for other users that are in the same partition behind this message.
*** Using key order concurrent processing will allow notifications to proceed while this message either slowly succeeds or times out and retires.
** Slow GPS tracking system (slow HTTP service interfaces that can scale horizontally)
*** GPS tracking messages from 100,000 different field devices pour through at a high rate into an input topic.
*** For each message, the GPS location coordinates is checked to be within allowed ranges using a legacy HTTP services, dictated by business rules behind the service.
*** The service takes 50ms to process each message, however can be scaled out horizontally without restriction.
*** The input topic only has 10 partitions and for various reasons (see above) cannot be changed.
*** With the vanilla consumer, messages on each partition must be consumed one after the other in serial order.
*** The maximum rate of message processing is then:
+
`1 second / 50 ms * 10 partitions = 200 messages per second.`
*** By using this library, the 10 partitions can all be processed in key order a potential.
+
`1 second / 50ms × 100,000 keys = 2,000,000 messages per second`
+
While the HTTP system probably cannot handle 2,000,000 messages per second, more importantly, your system is no longer the bottleneck.

** Slow CPU bound model processing for fraud prediction
*** Consider a system where message data is passed through a fraud prediction model which takes CPU cycles, instead of an external system being slow.
*** We can scale easily the number of CPUs on our virtual machine where the processing is being run, but we choose not to scale the partitions or consumers (see above).
*** By deploying onto machines with far more CPUs available, we can run our prediction model massively parallel, increasing our throughput and reducing our end-to-end response times.
* Spikey load with latency sensitive non-functional requirements
** An upstream system regularly floods our input topic daily at close of business with settlement totals data from retail outlets.
*** Situations like this are common where systems are designed to comfortably handle average day time load, but are not provisioned to handle sudden increases in traffic as they don't happen often enough to justify the increased spending on processing capacity that would otherwise remain idle.
*** Without adjusting the available partitions or running consumers, we can reduce our maximum end-to-end latency and increase throughout to get our global days outlet reports to division managers so action can be taken, before close of business.
** Natural consumer behaviour
*** Consider scenarios where bursts of data flooding input topics are generated by sudden user behaviour such as sales or television events ("Oprah" moments).
*** For example, an evening, prime-time game show on TV where users send in quiz answers on their devices.
The end-to-end latency of the responses to these answers needs to be as low as technically possible, even if the processing step is quick.
*** Instead of a vanilla client where each user response waits in a virtual queue with others to be processed, this library allows every single response to be processed in parallel.
* Legacy partition structure
** Any existing setups where we need higher performance eihter in throughput or latency where there are not enough partitions for needed concurrency level, the tool can be applied.
* Partition overloaded brokers
** Clusters with under-provisioned hardware and with too many partitions already - where we cannot expand partitions even if were able to.
** Similar to the above, but from the operations perspective, our system is already over partitioned, perhaps in order to support existing parallel workloads which aren't using the tool (and so need large numbers of partitions).
** We encourage our development teams to migrate to the tool, and then being a process of actually __lowering__ the number of partitions in our partitions in order to reduce operational complexity, improve reliability and perhaps save on infrastructure costs.
* Server side resources are controlled by a different team we can't influence
** The cluster our team is working with is not in our control, we cannot change the partition setup, or perhaps even the consumer layout.
** We can use the tool ourselves to improve our system performance without touching the cluster / topic setup.
* Kafka Streams app that had a slow stage
** We use Kafka Streams for our message processing, but one of it's steps have characteristics of the above and we need better performance. We can break out as described below into the tool for for processing that step, then return to the Kafka Streams context.
* Provisinoing extra machines (either virtual machines or real machines) to run multiple clients has a cost, using this library instead avoids the need for extra instances to be deployed in any respect.


=== Background

The core Kafka consumer client gives you a batch of messages to process one at a time.
Processing these in parallel on thread pools is difficult, particularly due to ensuring correctness of offset management, especially when trying to process concurrent by message key.

You also need to manage your consume loop, and commit transactions properly if using Exactly Once semantics.

This wrapper library for the core Kafka client handles all this for you, you just supply your processing function.

A very common example of a scenario where this helps tremulously, is in "slow consumer" situations, where processing a single message in your topic is slow, but the messages aren't so dependent on each other.Slow processing can be typically CPU bound, IO bound or bound by third party.The VertX extension to this library supplies many non-blocking interfaces allowing you to achieve high levels of concurrency easily.

== Support and Issues

This library is experimental, and Confluent does not currently offer support for this library.
If you encounter any issues, or have any suggestions or future requests, please create issues in the link:./issues[github issue tracker].

We are very interested to hear about your experiences!
You can also reach out to the maintainer via link:mailto:antony@confluent.io[email] or https://twitter.com/psynikal[twitter].

== License

This library is copyright Confluent Inc, and licensed under the XXX.

== Testing

The project has good automated test coverage, of all features.
Including integration tests running against real Kafka broker and database.
If you want to run the tests yourself, clone the repository and run the command: `mvn test`. The tests require an active docker server on `localhost`.

== Feature List
* Have massively parallel consumption processing without running hundreds or thousands of
** Kafka consumer clients
** topic partitions
+
without operational burden or harming the clusters performance
* Client side queueing system on top of Apache Kafka consumer
** Efficient individual message acknowledgement system (without local or third system state) to massively reduce message replay upon failure
* Per `key` concurrent processing, per partition and unordered message processing
* Offsets committed correctly, in order, of only processed messages, regardless of concurrency level or retries
* Vert.x non-blocking library integration (HTTP currently)
* Fair partition traversal
* Zero~ dependencies (`Slf4j` and `Lombok`) for the core module
* Throttle control and broker liveliness management
* Clean draining shutdown cycle

image:https://codecov.io/gh/astubbs/async-consumer/branch/master/graph/badge.svg["Coverage",https://codecov.io/gh/astubbs/async-consumer]
image:https://travis-ci.com/astubbs/async-consumer.svg?branch=master["Build Status", link="https://travis-ci.com/astubbs/async-consumer"]

And more <<roadmap,to come>>!

== Performance

In the best case, you don't care about ordering at all.In which case, the degree of concurrency achievable is simply set by max thread and concurrency settings, or with the VertX extension, the Vertx Vertical being used - e.g. non-blocking HTTP calls.

For example, instead of having to run 1,000 consumer to process 1,000 messages at the same time, we can process all 1,000 concurrently on a single consumer instance.

More typically though you probably still want the per key ordering grantees that Kafka provides.
For this there is the per key ordering setting.
This will limit the library from processing any message at the same time or out of order, if they have the same key.

Massively reduce message processing latency regardless of partition count for spikey workloads where there is good key distribution.
Eg 100,000 “users” all trigger an action at once.
As long as the processing layer can handle the load horizontally (e.g auto scaling web service), per message latency will be massively decreased, potentially down to the time for processing a single message, if the integration point can handle the concurrency.

For example, if you have a key set of 10,000 unique keys, and you need to call an http end point to process each one, you can use the per key order setting, and in the best case the system will process 10,000 at the same time using the non-blocking Vertx HTTP client library.
The user just has to provide a function to extract from the message the HTTP call parameters and construct the HTTP request object.

=== Illustrative Performance Example
.(see link:./parallel-consumer-core/src/test-integration/java/io/confluent/parallalconsumer/integrationTests/VolumeTests.java[VolumeTests.java])

This performance comparison results below, even though are based on real performance measurement results, are for illustrative purposes.
To see how the performance of the tool is related to instance counts, partition counts, key distribution and how it would relate to the vanilla client.
Actual results will vary wildly depending upon the setup being deployed into.

For example, if you have hundreds of thousands of keys in your topic, randomly distributed, even with hundreds of partitions, with only a handful of this wrapper deployed, you will probably see many orders of magnitude performance improvements - massively out performing dozens of vanila Kafka consumer clients.

image::https://docs.google.com/spreadsheets/d/e/2PACX-1vQffkAFG-_BzH-LKfGCVnytdzAHiCNIrixM6X2vF8cqw2YVz6KyW3LBXTB-lVazMAJxW0UDuFILKvtK/pubchart?oid=1691474082&amp;format=image[]

image::https://docs.google.com/spreadsheets/d/e/2PACX-1vQffkAFG-_BzH-LKfGCVnytdzAHiCNIrixM6X2vF8cqw2YVz6KyW3LBXTB-lVazMAJxW0UDuFILKvtK/pubchart?oid=1161363385&format=image[]

image::https://docs.google.com/spreadsheets/d/e/2PACX-1vQffkAFG-_BzH-LKfGCVnytdzAHiCNIrixM6X2vF8cqw2YVz6KyW3LBXTB-lVazMAJxW0UDuFILKvtK/pubchart?oid=938493158&format=image[]

As an illustrative example of relative performance, given:

* A random processing time between 0 and 5ms
* 10,000 messages to process
* A single partition (simplifies comparison - a topic with 5 partitions is the same as 1 partition with a keyspace of 5)
* Default `ParallelConsumerOptions`
** maxUncommittedMessagesToHandle = 1000
** maxConcurrency = 100
** numberOfThreads = 16

.Comparative performance of order modes and key spaces
[cols="1,1,1,3", options="header"]
|===
|Ordering
|Number of keys
|Duration
|Note

|Partition
|20 (not relevant)
|22.221s
|This is the same as a single partition with a single normal serial consumer, as we can see: 2.5ms avg processing time * 10,000 msg / 1000ms = ~25s.

|Key
|1
|26.743s
|Same as above

|Key
|2
|13.576s
|

|Key
|5
|5.916s
|

|Key
|10
|3.310s
|

|Key
|20
|2.242s
|

|Key
|50
|2.204s
|

|Key
|100
|2.178s
|

|Key
|1,000
|2.056s
|

|Key
|10,000
|2.128s
|As key space is t he same as the number of messages, this is similar (but restricted by max concurrency settings) as having a *single consumer* instance and *partition* _per key_. 10,000 msgs * avg processing time 2.5ms = ~2.5s.

|Unordered
|20 (not relevant)
|2.829s
|As there is no order restriction, this is similar (but restricted by max concurrency settings) as having a *single consumer* instance and *partition* _per key_. 10,000 msgs * avg processing time 2.5ms = ~2.5s.
|===

== Usage

=== Maven

Where `${project.version}` is the version to be used.

.Core Module Dependency
[source,xml,indent=0]
        <dependency>
            <groupId>io.confluent.csid.asyncconsumer</groupId>
            <artifactId>async-consumer-core</artifactId>
            <version>${project.version}</version>
        </dependency>

.Vert.x Module Dependency
[source,xml,indent=0]
        <dependency>
            <groupId>io.confluent.csid.asyncconsumer</groupId>
            <artifactId>async-consumer-vertx</artifactId>
            <version>${project.version}</version>
        </dependency>


.Repository
[source,xml,indent=0]
        <repository>
            <name>Confluent</name>
            <id>confluent</id>
            <url>http://packages.confluent.io/maven/</url>
        </repository>

=== Common Preparation

.Setup the async client
[source,java,indent=0]
----
        var options = AsyncConsumerOptions.builder()
                .ordering(KEY) // <1>
                .maxConcurrency(1000) // <2>
                .maxUncommittedMessagesToHandlePerPartition(10000) // <3>
                .build();

        Consumer<String, String> kafkaConsumer = getKafkaConsumer(); // <4>
        kafkaConsumer.subscribe(UniLists.of(inputTopic)); // <5>

        return new AsyncConsumer<>(kafkaConsumer, getKafkaProducer(), options);
----
<1> Choose your ordering type, `KEY` in this case.
This ensures maximum concurrency, while ensuring messages are processed and committed in `KEY` order, making sure no offset is committed unless all offsets before it in it's partition, are completed also.
<2> The maximum number of concurrent processing operations to be performing at any given time
<3> Regardless of the level of concurrency, don't have more than this many messages uncomitted at any given time.
Also, because the library coordinates offsets, `enable.auto.commit` must be disabled in your consumer.
<4> Setup your consumer client as per normal
<5> Setup your topic subscriptions

NOTE: Because the library coordinates offsets, `enable.auto.commit` must be disabled.

After this setup, one then has the choice of async interfaces:

* `ParallelConsumer`
* `VertxParallelConsumer`
* `StreamingParallelConsumer`
* `StreamingParallelVertxConsumer`


=== Core

==== Simple Message Process

This is the only thing you need to do, in order to get massively concurrent processing in your code.

.Usage - print message content out to the console in parallel
[source,java,indent=0]
        asyncConsumer.asyncPoll(record -> {
            log.info("Concurrently processing a record: {}", record);
        });

See the link:./parallel-consumer-examples/parallel-consumer-example-core/src/main/java/io/confluent/parallalconsumer/examples/core/CoreApp.java[core example] project, and it's test.

==== Process and Produce a Response Message

This interface allows you to process your message, then publish back to the broker zero, one or more result messages.
You can also optionally provide a callback function to be run after the message(s) is(are) successfully published to the broker.

.Usage - print message content out to the console in parallel
[source,java,indent=0]
        asyncConsumer.asyncPollAndProduce((record) -> {
            var result = processBrokerRecord(record);
            ProducerRecord<String, String> produceRecord =
                    new ProducerRecord<>(outputTopic, "a-key", result.payload);
            return List.of(produceRecord);
        }, (consumeProduceResult) -> {
            log.info("Message {} saved to broker at offset {}",
                    consumeProduceResult.getOut(),
                    consumeProduceResult.getMeta().offset());
        });


==== Callbacks vs Streams

You have the option to either use callbacks to be notified of events, or use the `Streaming` versions of the API, which use the `java.util.stream.Stream` system:

* `StreamingParallelConsumer`
* `StreamingParallelVertxConsumer`

In future versions, we plan to look at supporting other streaming systems like https://github.com/ReactiveX/RxJava[RxJava] via modules.

=== HTTP with the VertX Module

.Call an HTTP end point for each message usage
[source,java,indent=0]
----
        var resultStream = async.vertxHttpReqInfoStream(record -> {
            log.info("Concurrently constructing and returning RequestInfo from record: {}", record);
            Map params = UniMaps.of("recordKey", record.key(), "payload", record.value());
            return new RequestInfo("localhost", "/api", params); // <1>
        });
----
<1> Simply return an object representing the request, the Vert.x HTTP engine will handle the rest, using it's non-blocking engine

See the link:./parallel-consumer-examples/parallel-consumer-example-vertx/src/main/java/io/confluent/parallalconsumer/examples/vertx/VertxApp.java[Vert.x example] project, and it's test.

=== Kafka Streams Concurrent Processing

.Preprocess in Kafka Streams, then process concurrently
[source,java,indent=0]
----
    void run() {
        preprocess(); // <1>
        concurrentProcess(); // <2>
    }

    void preprocess() {
        StreamsBuilder builder = new StreamsBuilder();
        builder.<String, String>stream(inputTopic)
                .mapValues((key, value) -> {
                    log.info("Streams preprocessing key: {} value: {}", key, value);
                    return String.valueOf(value.length());
                })
                .to(outputTopicName);

        startStreams(builder.build());
    }

    void startStreams(Topology topology) {
        streams = new KafkaStreams(topology, getStreamsProperties());
        streams.start();
    }

    void concurrentProcess() {
        setupAsyncConsumer();

        asyncConsumer.asyncPoll(record -> {
            log.info("Concurrently processing a record: {}", record);
            messageCount.getAndIncrement();
        });
    }
----
<1> Setup your Kafka Streams stage as per normal, performing any type of preprocessing in Kafka Streams
<2> For the slow consumer part of your Topology, drop down into the async consumer, and use massive concurrency

See the link:./parallel-consumer-examples/parallel-consumer-example-streams/src/main/java/io/confluent/parallalconsumer/examples/streams/StreamsApp.java[Kafka Streams example] project, and it's test.

== Order enforced commit sync per partition

The user has the option to either choose ordered, or unordered message processing.

Either in `ordered` or `unordered` processing, the system will only commit offsets for messages which have been successfully processed.

Choose either, `ordered` processing means that processing of a given partition won't advance until each message has been successfully process.
This can hold up a partition, but ensures process order matches partition order.

TIP: `Unordered` processing is highly concurrent processing per partition, `ordered` is not.

CAUTION: `Unordered` processing could cause problems for third party integration where ordering by key is required.

CAUTION: Beware of third party systems which are not idempotent, or are key order sensitive.

=== Unordered

Unordered processing is where there is no such restriction on there being multiple messages processed per partition.
However, regardless of how far along the partition the processing progresses, the earliest outstanding message will block committing of offsets.

WARNING: If the system fails with many messages processed ahead of a single old message, ALL these messages will be processed again.

=== Ordered by Partition

At most only one message from any given input partition will be in flight at any given time.
This means that concurrent processing is restricted to the number of input partitions.

The advantage of ordered processing mode, is that for an input of 1000 partitions, you do not need to run 1000 application instances or threads, to process the partitions in parallel.

=== Ordered by Key

Most similar to ordered by partition, this mode ensures process ordering by key.

The advantage of this mode, is that a given input topic may not have many partitions, it may have a ~large number of keys.
Each of these key/message sets can actually possibly be processed concurrently, bringing concurrent processing to a per key level, without having to increase the number of input partitions, whilst keeping key ordering for the integrated systems.

And as usual, the order of offset commit's will be correct such that under failure, the system will resume from the most recently committed message in the input partitions.

CAUTION: Beware of retries, idempotency and rollbacks

=== Retries and Ordering

Even during retries, offsets will always be committed only after successful processing, and in order.

During `ordered` processing, retries will cause a partitions messages to be held up either until the message is given up on and sent to the DLQ.

== Result Models

* Void

Processing is complete simply when your provided function finishes, and the offsets are committed.

* Streaming User Results

When your function is actually run, a result object will be streamed back to your client code, with information about the operation completion.

* Streaming Message Publishing Results

After your operation completes, you can also choose to publish a result message back to Kafka.
The message publishing metadata can be streamed back to your client code.

== Apache Kafka EoS Transaction Model

Optionally, the user can enable AK EoE mode.
This causes all messages produced as a result of processing a message to be committed within a transaction.
This means that even under failure, at least for the Kafka output topic, the results will exist exactly once.

CAUTION: This cannot be true for any externally integrated third party system, unless that system is Idempotent.

[[streams-usage]]
== Using with Kafka Streams

Kafka Streams (KS) doesn't yet (https://cwiki.apache.org/confluence/display/KAFKA/KIP-311%3A+Async+processing+with+dynamic+scheduling+in+Kafka+Streams[KIP-311],
https://cwiki.apache.org/confluence/display/KAFKA/KIP-408%3A+Add+Asynchronous+Processing+To+Kafka+Streams[KIP-408]) have async processing of messages.
However, any given preprocessing can be done in KS, preparing the messages.
One can then use this library to consume from an input topic, produced by KS to process the messages in parallel.

[[roadmap]]
== Roadmap

=== Short Term - What we're working on nowish ⏰

* Producer is optional
* Transactions optional
* Depth~ first or breadth first partition traversal
* JavaRX and other streaming modules

=== Medium Term - What's up next ⏲

* Automatic fanout (automatic selection of concurrency level based on downstream back pressure)
* Support for general Vert.x Verticles (non-blocking libraries)
* Dead Letter Queue (DLQ) handling
* Non-blocking I/O work management
** More customisable handling of HTTP interactions
** Chance to batch multiple consumer records into a single or multiple http request objects
* Support https://jitpack.io/ version
* Distributed tracing integration
* Metrics

=== Long Term - The future ☁️

* Apache Kafka KIP?
* Call backs only offset has been committed
* resilience4j example / integration


== Usage Requirements

* Client side
** JDK 8
** SLF4J
** Apache Kafka (AK) Client libraries 2.5
** Supports all features of the AK client (e.g. security setups, schema registry etc)
** For use with Streams, see <<streams-usage>> section
** For use with Connect:
*** Source: simply consume from the topic that your Connect plugin is publishing to
*** Sink: use the `asyncPollAndProduce` API and publish the records to the topic that the connector is sinking from
* Server side
** Should work with any cluster that the linked AK client library works with
*** If using EoS/Transactions, needs a cluster setup that supports eos/transactions

== Development Information

=== Requirements

* Uses https://projectlombok.org/setup/intellij[Lombok], if you're using IntelliJ Idea, get the https://plugins.jetbrains.com/plugin/6317-lombok[plugin].
* Integration tests require a https://docs.docker.com/docker-for-mac/[running locally accessible Docker host].
* Has a Maven `profile` setup for IntelliJ Idea, but not Eclipse for example.

=== Notes

The unit test code is set to run at a very high frequency, which can make it difficult to read debug logs (or impossible). If you want to debug the code or view the main logs, consider changing the below:

// replace with code inclusion from readme branch
.AsyncConsumerTestBase
[source]
----
AsyncConsumerTestBase#DEFAULT_BROKER_POLL_FREQUENCY_MS
AsyncConsumerTestBase#DEFAULT_COMMIT_INTERVAL_MAX_MS
----

=== Readme

The `README` uses a special https://github.com/whelk-io/asciidoc-template-maven-plugin/pull/25[custom maven processor plugin] to import live code blocks into the root readme, so that GitHub can show the real code as includes in the `README`.
This is because GitHub https://github.com/github/markup/issues/1095[doesn't properly support the _include_ directive].

The source of truth readme is in link:./src/docs/README.adoc[].

=== Maven targets

[qanda]
Compile and run all tests::
`mvn verify`

Run tests excluding the integration tests::
`mvn test`

Run all tests::
`mvn verify`

Run any goal skipping tests (replace `<goalName>` e.g. `install`)::
`mvn <goalName> -DskipTests`

See what profiles are active::
`mvn help:active-profiles`

See what plugins or dependencies are avaible to be updated::
`mvn versions:display-plugin-updates versions:display-property-updates versions:display-dependency-updates`

==== Integration Testing with TestContainers
//https://github.com/confluentinc/schroedinger#integration-testing-with-testcontainers

We use the excellent https://testcontainers.org[Testcontainers] library for integration testing with JUnit.

To speed up test execution, you can enable container reused across test runs by setting the following in your https://www.testcontainers.org/features/configuration/[`~/.testcontainers.properties` file]:

[source]
----
testcontainers.reuse.enable=true
----

This will leave the container running after the JUnit test is complete for reuse by subsequent runs.

> NOTE: The container will only be left running if it is not explicitly stopped by the JUnit rule.
> For this reason, we use a variant of the https://www.testcontainers.org/test_framework_integration/manual_lifecycle_control/#singleton-containers[singleton container pattern]
> instead of the JUnit rule.

Testcontainers detects if a container is reusable by hashing the container creation parameters from the JUnit test.
If an existing container is _not_ reusable, a new container will be created **but the old container will not be removed**.

Target | Description
--- | ---
`testcontainers-list` | List all containers labeled as testcontainers
`testcontainers-clean` | Remove all containers labeled as testcontainers

.Stop and remove all containers labeled with `org.testcontainers=true`
[source,bash]
----
docker container ls --filter 'label=org.testcontainers=true' --format '{{.ID}}' \
| $(XARGS) docker container rm --force
----

.List all containers labeled with `org.testcontainers=true`
[source,bash]
----
docker container ls --filter 'label=org.testcontainers=true'
----

> NOTE: `testcontainers-clean` removes **all** docker containers on your system with the `io.testcontainers=true` label
> (including the most recent container which may be reusable).

See https://github.com/testcontainers/testcontainers-java/pull/1781[this testcontainers PR] for details on the reusable containers feature.

== Implementation Details

=== Offset Map

==== Storage

* Runtime data model creates list of incomplete offsets
* Then builds a full complete / not complete bit map from the base offset to be comitted
* Dynamically switching storage
** encodes into a `BitSet`, and a `ByteBuffer`, then compresses both using zstd, then uses the smallest and tags as such in the encoded String
** Which is smallest can depend on the size and information density of the offset map
*** Smaller maps fit better into uncompressed `BitSets` ~(30 entry map bitset: compressed: 13 Bytes, uncompressed: 4 Bytes, compressed ByteBuffer: )
*** Larger maps are usually better in compressed `ByteBuffer` ~(13750 entry map bitset: compressed: 63 Bytes, uncompressed: 1719 Bytes, compressed ByteBuffer: 41 Bytes)
*** Completely random offset maps, compressed and uncompressed `BitSet` is roughly the same (2000 entries, uncompressed bitset: 250, compressed: 259, compressed bytes array: 477)
*** Very large maps (20,000 entries), a compressed `BitSet` seems to be significantly smaller again.
*** A large map with a single oustanding message (2000 entries, 1 outstanding at a random position) compressed `BitSet` and `ByteArray` come out the same (23 vs 22 bytes, vs 250 for uncompressed `BitSet`)
* Stores along with base offset for each partition, in the offset commitsync metadata string
* The offset commit metadata has a hardcoded limit of 4096 bytes
** Because of this, if our map doesn't fit into this, we have to drop it and not use it, loosing the shorter replay benefits
** Upon recovery or restart or partition reassignment, the bitmap will be loaded as empty, and all messages from the offset committed will be replayed
** Not being able to fit the map into the metadata, depends on how many partitions we're assigned (it's 4096 bytes total for all assigned partitions, not each unfortunately), and the information density in the map (i.e. a single not yet completed message in 4000 completed ones will be a tiny map and will fit hundreds of partitions))
*** To improve this situation, we could introduce the option of a dynamic backpressure system, where the size of the serialised offset map is monitored, and if it is growing too large, we pause work until it can fit again
** @see `kafka.coordinator.group.OffsetConfig#DefaultMaxMetadataSize = 4096`

== Attribution

http://www.apache.org/[Apache®], http://kafka.apache.org/[Apache Kafka], and http://kafka.apache.org/[Kafka®] are either registered trademarks or trademarks of the http://www.apache.org/[Apache Software Foundation] in the United States and/or other countries.
