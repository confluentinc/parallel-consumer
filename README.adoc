# async-consumer
:icons:
:toc: macro
:toclevels: 3
:numbered: 1
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

image:https://travis-ci.com/astubbs/async-consumer.svg?branch=master["Build Status", link="https://travis-ci.com/astubbs/async-consumer"] image:https://codecov.io/gh/astubbs/async-consumer/branch/master/graph/badge.svg["Coverage",https://codecov.io/gh/astubbs/async-consumer]

A concurrent / asynchronous messaging processing library for the Apache Kafka consumer/producer with non blocking IO extensions.

.Consume _concurrently_ with a *single* consumer instance:
[source,java]
asyncConsumer.asyncPoll(record -> {
    log.info("A record: {}", record);
});


toc::[]

== Motivation

The core Kafka consumer client gives you a batch of messages to process one at a time.Processing these in parallel on thread pools is difficult, particularly due to ensuring correctness of offset management, especially when trying to process concurrent by message key.

You also need to manage your consume loop, and commit transactions properly if using Exactly Once semantics.

This wrapper library for the core Kafka client handles all this for you, you just supply your processing function.

A very common example of a scenario where this helps tremulously, is in "slow consumer" situations, where processing a single message in your topic is slow, but the messages aren't so dependent on each other.Slow processing can be typicaly CPU bound, IO bound or bound by third party.The VertX extension to this library supplies many non-blocking interfaces allowing you to achieve high levels of concurrency easily.

== Feature List
* Have massively parallel consumption processing without running hundreds or thousands of
** Kafka consumer clients
** topic partitions
+
without operational burden or harming the clusters performance
* Per `key` concurrent processing, per partition and unordered message processing
* Offsets committed correctly, in order, of only processed messages, regardless of concurrency level or retries
* Vert.x non-blocking library integration (HTTP currently)
* Fair partition traversal
* Zero~ dependencies (`Slf4j` and `Lombok`) for the core module
* Throttle control and broker liveliness management
* Clean draining shutdown cycle

image:https://codecov.io/gh/astubbs/async-consumer/branch/master/graph/badge.svg["Coverage",https://codecov.io/gh/astubbs/async-consumer]
image:https://travis-ci.com/astubbs/async-consumer.svg?branch=master["Build Status", link="https://travis-ci.com/astubbs/async-consumer"]

And more <<roadmap,to come>>!

=== Performance

In the best case, you don't care about ordering at all.In which case, the degree of concurrency achievable is simply set by max thread and concurrency settings, or with the VertX extension, the Vertx Vertical being used - e.g. non-blocking HTTP calls.

For example, instead of having to run 1,000 consumer to process 1,000 messages at the same time, we can process all 1,000 concurrently on a single consumer instance.

More typically though you probably still want the per key ordering grantees that Kafka provides.
For this there is the per key ordering setting.
This will limit the library from processing any message at the same time or out of order, if they have the same key.

Massively reduce message processing latency regardless of partition count for spikey workloads where there is good key distribution.
Eg 100,000 “users” all trigger an action at once.
As long as the processing layer can handle the load horizontally (e.g auto scaling web service), per message latency will be massively decreased, potentially down to the time for processing a single message, if the integration point can handle the concurrency.

For example, if you have a key set of 10,000 unique keys, and you need to call an http end point to process each one, you can use the per key order setting, and in the best case the system will process 10,000 at the same time using the non-blocking Vertx HTTP client library.
The user just has to provide a function to extract from the message the HTTP call parameters and construct the HTTP request object.

==== Illustrative Performance Example
.(see link:./async-consumer-core/src/test-integration/java/io/confluent/csid/asyncconsumer/integrationTests/VolumeTests.java[VolumeTests.java])

As an illustrative example of relative performance, given:

* A random processing time between 0 and 5ms
* 10,000 messages to process
* A single partition (simplifies comparison - a topic with 5 partitions is the same as 1 partition with a keyspace of 5)
* Default `AsyncConsumerOptions`
** maxUncommittedMessagesToHandle = 1000
** maxConcurrency = 100
** numberOfThreads = 16

.Comparative performance of order modes and key spaces
[cols="1,1,1,3", options="header"]
|===
|Ordering
|Number of keys
|Duration
|Note

|Partition
|20 (not relevant)
|22.221s
|This is the same as a single partition with a single normal serial consumer, as we can see: 2.5ms avg processing time * 10,000 msg / 1000ms = ~25s.

|Key
|1
|26.743s
|Same as above

|Key
|2
|13.576s
|

|Key
|5
|5.916s
|

|Key
|10
|3.310s
|

|Key
|20
|2.242s
|

|Key
|50
|2.204s
|

|Key
|100
|2.178s
|

|Key
|1,000
|2.056s
|

|Key
|10,000
|2.128s
|As key space is t he same as the number of messages, this is similar (but restricted by max concurrency settings) as having a *single consumer* instance and *partition* _per key_. 10,000 msgs * avg processing time 2.5ms = ~2.5s.

|Unordered
|20 (not relevant)
|2.829s
|As there is no order restriction, this is similar (but restricted by max concurrency settings) as having a *single consumer* instance and *partition* _per key_. 10,000 msgs * avg processing time 2.5ms = ~2.5s.
|===

== Usage

=== Common Preparation

.Setup the async client
[source,java]
----
AsyncConsumer<String, String> setupAsync() {
    var options = AsyncConsumerOptions.builder()
            .ordering(KEY) <1>
            .maxConcurrency(1000) <2>
            .maxUncommittedMessagesToHandle(10000) <3>
            .build();

    Consumer<String, String> kafkaConsumer = getKafkaConsumer(); <4>
    kafkaConsumer.subscribe(List.of("input-topic")); <5>

    return new AsyncConsumer<>(kafkaConsumer, getKafkaProducer(), options);
}
----
<1> Choose your ordering type, `KEY` in this case.
This ensures maximum concurrency, while ensuring messages are processed and committed in `KEY` order, making sure no offset is committed unless all offsets before it in it's partition, are completed also.
<2> The maximum number of concurrent processing operations to be performing at any given time
<3> Regardless of the level of concurrency, don't have more than this many messages uncomitted at any given time.
Also, because the library coordinates offsets, `enable.auto.commit` must be disabled in your consumer.
<4> Setup your consumer client as per normal
<5> Setup your topic subscriptions

NOTE: Because the library coordinates offsets, `enable.auto.commit` must be disabled.

After this setup, one then has the choice of async interfaces:

* `AsyncConsumer`
* `VertxAsyncConsumer`
* `StreamingAsyncConsumer`
* `StreamingAsyncVertxConsumer`


=== Core

==== Simple Message Process

This is the only thing you need to do, in order to get massively concurrent processing in your code.

.Usage - print message content out to the console in parallel
[source,java]
async.asyncPoll(record -> {
        log.info("A record: {}", record);
    });

See the link:./async-consumer-examples/async-consumer-example-core/src/main/java/io/confluent/csid/asyncconsumer/examples/core/CoreApp.java[core example] project, and it's test.

==== Process and Produce a Response Message

This interface allows you to process your message, then publish back to the broker zero, one or more result messages.
You can also optionally provide a callback function to be run after the message(s) is(are) successfully published to the broker.

.Usage - print message content out to the console in parallel
[source,java]
async.asyncPollAndProduce((record) -> {
        var result = processBrokerRecord(record);
        ProducerRecord<String, String> produceRecord =
                    new ProducerRecord<>(OUTPUT_TOPIC, "a-key", result.payload);
        return List.of(produceRecord);
    }, (consumeProduceResult) -> {
        log.info("Message {} saved to broker at offset {}",
            consumeProduceResult.out.key,
            consumeProduceResult.meta.offset());
    });

==== Callbacks vs Streams

You have the option to either use callbacks to be notified of events, or use the `Streaming` versions of the API, which use the `java.util.stream.Stream` system:

* `StreamingAsyncConsumer`
* `StreamingAsyncVertxConsumer`

In future versions, we plan to look at supporting other streaming systems like https://github.com/ReactiveX/RxJava[RxJava] via modules.

=== HTTP with the VertX Module

.Call an HTTP end point for each message usage
[source,java]
----
async.vertxHttpReqInfoStream(record -> {
        Map params = Map.of("recordKey", record.key(), "payload", record.value());
        return new RequestInfo("myhost.com", "/api", params); <1>
    });
----
<1> Simply return an object representing the request, the Vert.x HTTP engine will handle the rest, using it's non-blocking engine

See the link:./async-consumer-examples/async-consumer-example-vertx/src/main/java/io/confluent/csid/asyncconsumer/examples/vertx/VertxApp.java[Vert.x example] project, and it's test.

=== Kafka Streams Concurrent Processing

.Preprocess in Kafka Streams, then process concurrently
[source,java]
----
void run() {
    preprocess(); <1>
    concurrentProcess(); <2>
}

void preprocess() {
    StreamsBuilder builder = new StreamsBuilder();
    builder.<String, String>stream("my-input-topic")
            .mapValues(value -> String.valueOf(value.length()))
            .to(outputTopicName);

    KafkaStreams streams = new KafkaStreams(builder.build(), getStreamsProperties());
    streams.start();
}

void concurrentProcess() {
    async.asyncPoll(record -> {
        log.info("A record: {}", record);
    });
}
----
<1> Setup your Kafka Streams stage as per normal, performing any type of preprocessing in Kafka Streams
<2> For the slow consumer part of your Topology, drop down into the async consumer, and use massive concurrency

See the link:./async-consumer-examples/async-consumer-example-streams/src/main/java/io/confluent/csid/asyncconsumer/examples/streams/StreamsApp.java[Kafka Streams example] project, and it's test.

== Order enforced commit sync per partition

The user has the option to either choose ordered, or unordered message processing.

Either in `ordered` or `unordered` processing, the system will only commit offsets for messages which have been successfully processed.

Choose either, `ordered` processing means that processing of a given partition won't advance until each message has been successfully process.
This can hold up a partition, but ensures process order matches partition order.

TIP: `Unordered` processing is highly concurrent processing per partition, `ordered` is not.

CAUTION: `Unordered` processing could cause problems for third party integration where ordering by key is required.

CAUTION: Beware of third party systems which are not idempotent, or are key order sensitive.

=== Unordered

Unordered processing is where there is no such restriction on there being multiple messages processed per partition.
However, regardless of how far along the partition the processing progresses, the earliest outstanding message will block committing of offsets.

WARNING: If the system fails with many messages processed ahead of a single old message, ALL these messages will be processed again.

=== Ordered by Partition

At most only one message from any given input partition will be in flight at any given time.
This means that concurrent processing is restricted to the number of input partitions.

The advantage of ordered processing mode, is that for an input of 1000 partitions, you do not need to run 1000 application instances or threads, to process the partitions in parallel.

=== Ordered by Key

Most similar to ordered by partition, this mode ensures process ordering by key.

The advantage of this mode, is that a given input topic may not have many partitions, it may have a ~large number of keys.
Each of these key/message sets can actually possibly be processed concurrently, bringing concurrent processing to a per key level, without having to increase the number of input partitions, whilst keeping key ordering for the integrated systems.

And as usual, the order of offset commit's will be correct such that under failure, the system will resume from the most recently committed message in the input partitions.

CAUTION: Beware of retries, idempotency and rollbacks

=== Retries and Ordering

Even during retries, offsets will always be committed only after successful processing, and in order.

During `ordered` processing, retries will cause a partitions messages to be held up either until the message is given up on and sent to the DLQ.

== Result Models

* Void

Processing is complete simply when your provided function finishes, and the offsets are committed.

* Streaming User Results

When your function is actually run, a result object will be streamed back to your client code, with information about the operation completion.

* Streaming Message Publishing Results

After your operation completes, you can also choose to publish a result message back to Kafka.
The message publishing metadata can be streamed back to your client code.

== Apache Kafka EoS Transaction Model

Optionally, the user can enable AK EoE mode.
This causes all messages produced as a result of processing a message to be committed within a transaction.
This means that even under failure, at least for the Kafka output topic, the results will exist exactly once.

CAUTION: This cannot be true for any externally integrated third party system, unless that system is Idempotent.

[[streams-usage]]
== Using with Kafka Streams

Kafka Streams (KS) doesn't yet (https://cwiki.apache.org/confluence/display/KAFKA/KIP-311%3A+Async+processing+with+dynamic+scheduling+in+Kafka+Streams[KIP-311],
https://cwiki.apache.org/confluence/display/KAFKA/KIP-408%3A+Add+Asynchronous+Processing+To+Kafka+Streams[KIP-408]) have async processing of messages.
However, any given preprocessing can be done in KS, preparing the messages.
One can then use this library to consume from an input topic, produced by KS to process the messages in parallel.

[[roadmap]]
== Roadmap

=== Short Term - What we're working on nowish ⏰

* Producer is optional
* Transactions optional
* Depth~ first or breadth first partition traversal
* JavaRX and other streaming modules

=== Medium Term - What's up next ⏲

* Automatic fanout (automatic selection of concurrency level based on downstream back pressure)
* Support for general Vert.x Verticles (non-blocking libraries)
* Dead Letter Queue (DLQ) handling
* Non-blocking I/O work management
** More customisable handling of HTTP interactions
** Chance to batch multiple consumer records into a single or multiple http request objects
* Support https://jitpack.io/ version
* Distributed tracing integration
* Metrics

=== Long Term - The future ☁️

* Apache Kafka KIP?
* Call backs only offset has been committed
* resilience4j example / integration

'''

== Usage Requirements

* Client side
** JDK 9 (8 coming)
** SLF4J
** Apache Kafka (AK) Client libraries 2.5
** Supports all features of the AK client (e.g. security setups, schema registry etc)
** For use with Streams, see <<streams-usage>> section
** For use with Connect:
*** Source: simply consume from the topic that your Connect plugin is publishing to
*** Sink: use the `asyncPollAndProduce` API and publish the records to the topic that the connector is sinking from
* Server side
** Should work with any cluster that the linked AK client library works with
*** If using EoS/Transactions, needs a cluster setup that supports eos/transactions

== Development Information

=== Requirements

* Uses https://projectlombok.org/setup/intellij[Lombok], if you're using IntelliJ Idea, get the https://plugins.jetbrains.com/plugin/6317-lombok[plugin].
* Integration tests require a https://docs.docker.com/docker-for-mac/[running locally accessible Docker host].
* Has a Maven `profile` setup for IntelliJ Idea, but not Eclipse for example.

=== Maven targets

[qanda]
Compile and run all tests::
`mvn verify`

Run tests excluding the integration tests::
`mvn test`

Run all tests::
`mvn verify`

Run any goal skipping tests (replace `<goalName>` e.g. `install`)::
`mvn <goalName> -DskipTests`

See what profiles are active::
`mvn help:active-profiles`

See what plugins or dependencies are avaible to be updated::
`mvn versions:display-plugin-updates versions:display-property-updates versions:display-dependency-updates`

==== Integration Testing with TestContainers
//https://github.com/confluentinc/schroedinger#integration-testing-with-testcontainers

We use the excellent [Testcontainers](https://testcontainers.org) library for integration testing with JUnit.

To speed up test execution, you can enable container reused across test runs by setting the
following in your [`~/.testcontainers.properties` file](https://www.testcontainers.org/features/configuration/):

[source]
----
testcontainers.reuse.enable=true
----

This will leave the container running after the JUnit test is complete for reuse by subsequent runs.

> NOTE: The container will only be left running if it is not explicitly stopped by the JUnit rule.
> For this reason, we use a variant of the [singleton container pattern](https://www.testcontainers.org/test_framework_integration/manual_lifecycle_control/#singleton-containers)
> instead of the JUnit rule.

Testcontainers detects if a container is reusable by hashing the container creation parameters
from the JUnit test.  If an existing container is _not_ reusable, a new container will be created
**but the old container will not be removed**.

Target | Description
--- | ---
`testcontainers-list` | List all containers labeled as testcontainers
`testcontainers-clean` | Remove all containers labeled as testcontainers

.Stop and remove all containers labeled with `org.testcontainers=true`
[source,bash]
----
docker container ls --filter 'label=org.testcontainers=true' --format '{{.ID}}' \
| $(XARGS) docker container rm --force
----

.List all containers labeled with `org.testcontainers=true`
[source,bash]
----
docker container ls --filter 'label=org.testcontainers=true'
----

> NOTE: `testcontainers-clean` removes **all** docker containers on your system with the `io.testcontainers=true` label
> (including the most recent container which may be reusable).

See [this testcontainers PR](https://github.com/testcontainers/testcontainers-java/pull/1781) for
details on the reusable containers feature.