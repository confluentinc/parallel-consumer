:project_root: ../../
:toc: macro

= Introducing Confluent Parallel Consumer

toc::[]

== (place marker: intro - the problem)

// == (Partition restriction)

The core Apache Kafka consumer client gives you a batch of messages to process, where each is typically processed one at a time.
Processing these in parallel on thread pools is difficult, particularly due to ensuring correctness of offset management, especially when trying to process concurrently by message key.

In Kafka, the numerator of the unit of concurrency and thus scaling out performance, is the number of partitions (degree of sharding).
Message processing ordering is also guaranteed by the partition construct - each message will be processed in the order they are written to the log partition

Currently, in Kafka 2.6, you cannot decrease the number of partitions at all without copying the topic data to a new topic with the desired number of partitions and dealing with the complexities involved in doing this, and you can’t increase in place without breaking your key ordering, otherwise you also must use the copy technique.

Because of this rigidity and complexity, users are required to consider carefully how many partitions will be required for their use cases at the time of their first production deployment for the topics.
Load testing your project helps get this right, but is not simple and has to be done for every new topic.

Sequential processing of messages for a single partition has a common performance issue, often referred to as the "slow consumer" scenario.
This is a situation where processing a single message in your topic is slow, causing all messages in line behind to be held up until processing is complete.
Slow processing can be typically CPU bound, IO bound or bound by third party system being integrated with, say a database or a webserver.

.The slow consumer situation with the raw Apache Kafka Consumer client
image::https://lucid.app/publicSegments/view/98ad200f-97b2-479b-930c-2805491b2ce7/image.png[]

Processing messages in parallel will avoid this problem.
However, correctly managing offset committing, asynchronous message processing results, retries, and if using Exactly Once semantics, committing transactions properly - is complicated to get right.
Also your use case may rely on the partition ordering guarantee that Kafka provides.

Introducing the experimental library the Confluent Parallel Consumer, from Confluent's Customer Solutions Innovation department.
This wrapper library for the core Kafka clients has an engine for processing the consumer's messages in parallel and managing transactions, you just supply your processing function.

Core to the library's features is also per key ordering guarantees meaning - one of the concurrency options is for messages of the same key to be processed strictly in partition order.
Most users will find this level of ordering will satisfy their use case, and mixed key partition ordering is not necessary, and so be able to take advantage of the parallel processing.

The system also uses a client queueing type of behaviour, where each offset is tracked individually for completion.
This is persisted to the broker, so after failure recovery or partition rebalancing, only messages which have not been completed correctly will be replayed, regardless of the actual consumer protocol committed offset.

.Example usage of the Parallel Consumer
image::https://lucid.app/publicSegments/view/2cb3b7e2-bfdf-4e78-8247-22ec394de965/image.png[]

To speed up the message processing system even more, the library includes an optional https://github.com/confluentinc/parallel-consumer/tree/master#74-http-with-the-vertx-module[VertX integration extension], which supplies many non-blocking interfaces allowing higher levels of concurrency without being restricted to thread pool sizes.

If your use case fits - this will allow you to process your messages _orders of magnitudes faster_, with much lower latency, *without* adjusting your partitions or managing more client instances.

Parallel Apache Kafka client wrapper with client side queueing, a simpler consumer/producer API with *key concurrency* and *extendable non-blocking IO* processing.

WARNING: This is not a Confluent supported product.
It is an experimental alpha stage accelerator.
See the https://github.com/confluentinc/parallel-consumer#2-support-and-issues[Support and Issues] section for more information.

As processing speed and response time isn't bound to the number of partitions in a topic anymore, go faster in
situations where;

* partition counts are fixed such as legacy setups
* production deployments where partitions can't otherwise be changed
* where brokers are overloaded with too many partitions already
* where the partition count is out of your control,
* where running large consumer groups
* many consumer clients isn't desirable

.Consume many messages _concurrently_ with a *single* consumer instance:
[source,java,indent=0]
----
include::{project_root}/parallel-consumer-examples/parallel-consumer-example-core/src/main/java/io/confluent/parallelconsumer/examples/core/CoreApp.java[tag=example]
----

// === FAQ

[qanda]
Why not just run more consumers?::
The typical way to address performance issues in a Kafka system, is to increase the number of consumers reading from a topic.
This is effective in many situations, but falls short in a lot too.
* Primarily: You cannot use more consumers than you have partitions available to read from.
+
For example, if you have a topic with five partitions, you cannot use more than five consumers to read from it.

** You can use this library to process messages concurrently up to the number of keys available, without worrying about increasing partition counts.
** You can increase partition counts by first copying all topic data to a new topic, but that doubles the data requirements for the topic, adds another processing step that has to be monitored, and increases minimum latency in end to end processing.
Then your level of concurrency (and thus latency and throughput) is then _still_ restricted to the number of partitions and consumer instances available.
** Large numbers of partitions in a Kafka broker system is one of the primary causes of instability and performance problems with over burdened cluster systems.
* Running more extra consumers has resource implications - each consumer takes up resources on both the client and broker side.
Each consumer adds a lot of overhead in terms of memory, CPU, and network bandwidth.
* Large consumer groups (especially many large groups) can cause a lot of strain on the consumer group coordination system, such as rebalance storms.
* Even with several partitions, you cannot achieve the performance levels obtainable by *per-key* ordered or unordered concurrent processing

Why not run more consumers __within__ your application instance?::
* This is in some respects a slightly easier way of running more consumer instances, and in others a more complicated way.
However, you are still restricted by all the per consumer restrictions as described above.

Why not use the Vert.x library yourself in your processing loop?::
* Vert.x us used in this library to provide a non-blocking IO system in the message processing step.
Using Vert.x without using this library with *ordered* processing requires dealing with the quite complicated, and not straight forward, aspect of handling offset commits with Vert.x asynchronous processing system.
+
*Unordered* processing with Vert.x is somewhat easier, however offset management is still quite complicated, to  ensure that offsets are not committed, which would cause other not-yet-processed offsets to be incorrectly committed.
This library handles offset commits for both ordered and unordered processing cases.


== Scenarios

Below are some real world use cases which illustrate concrete situations where the described advantages massively improve performance.

* Slow consumer in transactional systems (online vs offline or reporting systems)
** Notification system:
+
*** Notification processing system which sends push notifications to a user to acknowledge a two-factor authentication request on their mobile and authorising a login to a website, requires optimal end-to-end latency for a good user experience.
*** A specific message in this queue uncharacteristically takes a long time to process because the third party system is sometimes unpredictably slow to respond and so holds up the processing for *ALL* other notifications for other users that are in the same partition behind this message.
*** Using key order concurrent processing will allow notifications to proceed while this message either slowly succeeds or times out and retires.
** Slow GPS tracking system (slow HTTP service interfaces that can scale horizontally)
*** GPS tracking messages from 100,000 different field devices pour through at a high rate into an input topic.
*** For each message, the GPS location coordinates is checked to be within allowed ranges using a legacy HTTP services, dictated by business rules behind the service.
*** The service takes 50ms to process each message, however can be scaled out horizontally without restriction.
*** The input topic only has 10 partitions and for various reasons (see above) cannot be changed.
*** With the vanilla consumer, messages on each partition must be consumed one after the other in serial order.
*** The maximum rate of message processing is then:
+
`1 second / 50 ms * 10 partitions = 200 messages per second.`
*** By using this library, the 10 partitions can all be processed in key order.
+
`1 second / 50ms × 100,000 keys = 2,000,000 messages per second`
+
While the HTTP system probably cannot handle 2,000,000 messages per second, more importantly, your system is no longer the bottleneck.

** Slow CPU bound model processing for fraud prediction
*** Consider a system where message data is passed through a fraud prediction model which takes CPU cycles, instead of an external system being slow.
*** We can scale easily the number of CPUs on our virtual machine where the processing is being run, but we choose not to scale the partitions or consumers (see above).
*** By deploying onto machines with far more CPUs available, we can run our prediction model massively parallel, increasing our throughput and reducing our end-to-end response times.
* Spikey load with latency sensitive non-functional requirements
** An upstream system regularly floods our input topic daily at close of business with settlement totals data from retail outlets.
*** Situations like this are common where systems are designed to comfortably handle average day time load, but are not provisioned to handle sudden increases in traffic as they don't happen often enough to justify the increased spending on processing capacity that would otherwise remain idle.
*** Without adjusting the available partitions or running consumers, we can reduce our maximum end-to-end latency and increase throughout to get our global days outlet reports to division managers so action can be taken, before close of business.
** Natural consumer behaviour
*** Consider scenarios where bursts of data flooding input topics are generated by sudden user behaviour such as sales or television events ("Oprah" moments).
*** For example, an evening, prime-time game show on TV where users send in quiz answers on their devices.
The end-to-end latency of the responses to these answers needs to be as low as technically possible, even if the processing step is quick.
*** Instead of a vanilla client where each user response waits in a virtual queue with others to be processed, this library allows every single response to be processed in parallel.
* Legacy partition structure
** Any existing setups where we need higher performance either in throughput or latency where there are not enough partitions for needed concurrency level, the tool can be applied.
* Partition overloaded brokers
** Clusters with under-provisioned hardware and with too many partitions already - where we cannot expand partitions even if we were able to.
** Similar to the above, but from the operations perspective, our system is already over partitioned, perhaps in order to support existing parallel workloads which aren't using the tool (and so need large numbers of partitions).
** We encourage our development teams to migrate to the tool, and then being a process of actually __lowering__ the number of partitions in our partitions in order to reduce operational complexity, improve reliability and perhaps save on infrastructure costs.
* Server side resources are controlled by a different team we can't influence
** The cluster our team is working with is not in our control, we cannot change the partition setup, or perhaps even the consumer layout.
** We can use the tool ourselves to improve our system performance without touching the cluster / topic setup.
* Kafka Streams app that had a slow stage
** We use Kafka Streams for our message processing, but one of it's steps have characteristics of the above and we need better performance.
We can break out as described below into the tool for processing that step, then return to the Kafka Streams context.
* Provisioning extra machines (either virtual machines or real machines) to run multiple clients has a cost, using this library instead avoids the need for extra instances to be deployed in any respect.


== Performance

In the best case, you don't care about ordering at all.
In which case, the degree of concurrency achievable is simply set by max thread and concurrency settings, or with the VertX extension, the Vertx Vertical being used - e.g. non-blocking HTTP calls.

For example, instead of having to run 1,000 consumers to process 1,000 messages at the same time, we can process all 1,000 concurrently on a single consumer instance.

More typically though you probably still want the per key ordering grantees that Kafka provides.
For this there is the *per key ordering* setting.
This will limit the library from processing any message at the same time or out of order, if they have the same key.

Message processing latency is also massively reduced, regardless of partition count, where there is spikyey distribution - important for spikey workloads.
Eg 100,000 “users” all trigger an action at once.
As long as the processing layer can handle the load horizontally (e.g auto scaling web service), per message latency will be massively decreased, potentially down to the time for processing a single message, if the integration point can handle the concurrency.

For example, if you have a key set of 10,000 unique keys, and you need to call an http end point to process each one, you can use the per key order setting, and in the best case the system will process 10,000 at the same time using the non-blocking Vertx HTTP client library.
The user just has to provide a function to extract from the message the HTTP call parameters and construct the HTTP request object.

=== Illustrative Performance Example
.(see link:./parallel-consumer-core/src/test-integration/java/io/confluent/parallelconsumer/integrationTests/VolumeTests.java[VolumeTests.java])

This performance comparison results below, even though are based on real performance measurement results, are illustrative only - to see how the performance of the tool is related to instance counts, partition counts, key distribution and how it would relate to the vanilla client.
Actual results will vary wildly depending upon the setup being deployed, settings used and the characteristics of the input data.

As an example, if you have hundreds of thousands of keys in your topic, randomly distributed, even with hundreds of partitions, with only a handful of this wrapper deployed, you will probably see many orders of magnitude performance improvements - massively out performing dozens of vanilla Kafka consumer clients.

.Time taken to process a large number of messages with a Single Parallel Consumer vs a single Kafka Consumer, for different key space sizes. As the number of unique keys in the data set increases, the key ordered Parallel Consumer performance starts to approach that of the unordered Parallel Consumer. The raw Kafka consumer performance remains unaffected by the key distribution.
image::https://docs.google.com/spreadsheets/d/e/2PACX-1vQffkAFG-_BzH-LKfGCVnytdzAHiCNIrixM6X2vF8cqw2YVz6KyW3LBXTB-lVazMAJxW0UDuFILKvtK/pubchart?oid=1691474082&amp;format=image[]

.Consumer group size effect on total processing time vs a single Parallel Consumer. As instances are added to the consumer group, it's performance starts to approach that of the single instance Parallel Consumer. Key ordering is faster than partition ordering, with unordered being the fastest.
image::https://docs.google.com/spreadsheets/d/e/2PACX-1vQffkAFG-_BzH-LKfGCVnytdzAHiCNIrixM6X2vF8cqw2YVz6KyW3LBXTB-lVazMAJxW0UDuFILKvtK/pubchart?oid=938493158&format=image[]

.Consumer group size effect on message latency vs a single Parallel Consumer. As instances are added to the consumer group, it's performance starts to approach that of the single instance Parallel Consumer.
image::https://docs.google.com/spreadsheets/d/e/2PACX-1vQffkAFG-_BzH-LKfGCVnytdzAHiCNIrixM6X2vF8cqw2YVz6KyW3LBXTB-lVazMAJxW0UDuFILKvtK/pubchart?oid=1161363385&format=image[]


== Usage

=== Common Preparation

.Setup the client
[source,java,indent=0]
----
include::{project_root}/parallel-consumer-examples/parallel-consumer-example-core/src/main/java/io/confluent/parallelconsumer/examples/core/CoreApp.java[tag=exampleSetup]
----
<1> Choose your ordering type, `KEY` in this case.
This ensures maximum concurrency, while ensuring messages are processed and committed in `KEY` order, making sure no offset is committed unless all offsets before it in it's partition, are completed also.
<2> The maximum number of concurrent processing operations to be performing at any given time
<3> Regardless of the level of concurrency, don't have more than this many messages uncommitted at any given time.
Also, because the library coordinates offsets, `enable.auto.commit` must be disabled in your consumer.
<4> Setup your consumer client as per normal
<5> Setup your topic subscriptions - (when using the `MockConsumer` you must use the `MockConsumer#assign` method)

After this setup, one then has the choice of interfaces:

* `ParallelConsumer`
* `VertxParallelConsumer`

=== Core

==== Simple Message Process

This is the only thing you need to do, in order to get massively concurrent processing in your code.

.Usage - print message content out to the console in parallel
[source,java,indent=0]
include::{project_root}/parallel-consumer-examples/parallel-consumer-example-core/src/main/java/io/confluent/parallelconsumer/examples/core/CoreApp.java[tag=example]

See the link:./parallel-consumer-examples/parallel-consumer-example-core/src/main/java/io/confluent/parallelconsumer/examples/core/CoreApp.java[core example] project, and it's test.

==== Process and Produce a Response Message

This interface allows you to process your message, then publish back to the broker zero, one or more result messages.
You can also optionally provide a callback function to be run after the message(s) is(are) successfully published to the broker.

.Usage - print message content out to the console in parallel
[source,java,indent=0]
include::{project_root}/parallel-consumer-examples/parallel-consumer-example-core/src/main/java/io/confluent/parallelconsumer/examples/core/CoreApp.java[tag=exampleProduce]


=== HTTP with the Vert.x Module

The library ships with an optional integration with Vert.x, making use of it's non-blocking IO systems to improve concurrency performance of processing calls without being bound to a thread pool size.

.Call an HTTP end point for each message usage
[source,java,indent=0]
----
include::{project_root}/parallel-consumer-examples/parallel-consumer-example-vertx/src/main/java/io/confluent/parallelconsumer/examples/vertx/VertxApp.java[tag=example]
----
<1> Simply return an object representing the request, the Vert.x HTTP engine will handle the rest, using it's non-blocking engine

See the link:{project_root}/parallel-consumer-examples/parallel-consumer-example-vertx/src/main/java/io/confluent/parallelconsumer/examples/vertx/VertxApp.java[Vert.x example] project, and it's test.

[[streams-usage-code]]
=== Kafka Streams Concurrent Processing

Kafka Streams (KS) doesn't yet (https://cwiki.apache.org/confluence/display/KAFKA/KIP-311%3A+Async+processing+with+dynamic+scheduling+in+Kafka+Streams[KIP-311],
https://cwiki.apache.org/confluence/display/KAFKA/KIP-408%3A+Add+Asynchronous+Processing+To+Kafka+Streams[KIP-408]) have parallel processing of messages.
However, any given preprocessing can be done in KS, preparing the messages.
One can then use this library to consume from an input topic, produced by KS to process the messages in parallel.

Use your Streams app to process your data first, then send anything needed to be processed concurrently to an output topic, to be consumed by the parallel consumer.

For a code example, see the <<streams-usage-code>> section.

.Example usage with Kafka Streams
image::https://lucid.app/publicSegments/view/43f2740c-2a7f-4b7f-909e-434a5bbe3fbf/image.png[Kafka Streams Usage]

.Preprocess in Kafka Streams, then process concurrently
[source,java,indent=0]
----
include::{project_root}/parallel-consumer-examples/parallel-consumer-example-streams/src/main/java/io/confluent/parallelconsumer/examples/streams/StreamsApp.java[tag=example]
----
<1> Setup your Kafka Streams stage as per normal, performing any type of preprocessing in Kafka Streams
<2> For the slow consumer part of your Topology, drop down into the parallel consumer, and use massive concurrency

See the link:{project_root}/parallel-consumer-examples/parallel-consumer-example-streams/src/main/java/io/confluent/parallelconsumer/examples/streams/StreamsApp.java[Kafka Streams example] project, and it's test.


== Ordering Guarantees

The user has the option to either choose key ordered, partition ordered, or unordered message processing.

=== Unordered

Unordered processing is where there is no restriction on the order of multiple messages processed per partition, allowing for highest level of concurrency.

=== Ordered by Partition

At most only one message from any given input partition will be in flight at any given time.
This means that concurrent processing is restricted to the number of input partitions.

The advantage of ordered processing mode, is that for an assignment of 1000 partitions to a single consumer, you do not need to run 1000 consumer instances or threads, to process the partitions in parallel.

Note that for a given partition, a slow processing message _will_ prevent messages behind it from being processed.
However, messages in other partitions assigned to the consumer _will_ continue processing.

=== Ordered by Key

Most similar to ordered by partition, this mode ensures process ordering by *key* (per partition).

The advantage of this mode, is that a given input topic may not have many partitions, it may have a ~large number of unique keys.
Each of these key -> message sets can actually be processed concurrently, bringing concurrent processing to a per key level, without having to increase the number of input partitions, whilst keeping strong ordering by key.

As usual, the offset tracking will be correct, regardless of the ordering of unique keys on the partition or adjacency to the committed offset, such that after failure or rebalance, the system will not replay messages already marked as successful.

=== Retries and Ordering

Even during retries, offsets will always be committed only after successful processing, and in order.


[[roadmap]]
== Roadmap

Take a look at the https://github.com/confluentinc/parallel-consumer#feature-list[full feature list], as well as what we have planned in the https://github.com/confluentinc/parallel-consumer/tree/master#roadmap[roadmap], such as other interfaces such as JavaRX integration, automatic scale up/down control based on throughput, more control over work management and more.


== Implementation Details

=== Core Architecture

Concurrency is controlled by the size of the thread pool (`worker pool` in the diagram).
Work is performed in a blocking manner, by the users submitted lambda functions.

.Core Architecture
image::https://lucid.app/publicSegments/view/320d924a-6517-4c54-a72e-b1c4b22e59ed/image.png[Core Architecture]

=== Vert.x Architecture

Concurrency is controlled by a max parallel requests setting, and work is performed asynchronously on the Vert.x engine by a core count thread pool using Vert.x asynchronous IO plugins (https://vertx.io/docs/vertx-core/java/#_verticles[verticles]).

.Vert.x Architecture
image::https://lucid.app/publicSegments/view/509df410-5997-46be-98e7-ac7f241780b4/image.png[Vert.x Architecture]

== What's next?

Go clone the https://github.com/confluentinc/parallel-consumer/[respoitory] and try it out in your projects! Use the Github issues system to send us any feedback or issues you have.

WARNING: This library is experimental, and Confluent does not currently offer support for this library.

If you encounter any issues, or have any suggestions or future requests, please create issues in the https://github.com/confluentinc/async-consumer/issues[github issue tracker].
Issues will be dealt with on a good faith, best efforts basis, by the small team maintaining this library.

We also encourage participation, so if you have any feature ideas etc, please get in touch, and we will help you work on submitting a PR!

For anything else, you can otherwise reach the primary maintainer via link:mailto:antony@confluent.io[email].
